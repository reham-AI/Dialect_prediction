{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AraBERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xgyE8GthSBT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    !nvidia-smi\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.12.2\n",
        "!pip install farasapy==0.0.14\n",
        "!pip install pyarabic==0.6.14\n",
        "!git clone https://github.com/aub-mind/arabert\n",
        "!pip install emoji==1.6.1\n",
        "!pip install sentencepiece==0.1.96"
      ],
      "metadata": {
        "id": "2Vs270WYhWsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "iHxSt63LhbDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset:\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        train: List[pd.DataFrame],\n",
        "        test: List[pd.DataFrame],\n",
        "        label_list: List[str],\n",
        "    ):\n",
        "        \"\"\"Class to hold and structure datasets.\n",
        "\n",
        "        Args:\n",
        "\n",
        "        name (str): holds the name of the dataset so we can select it later\n",
        "        train (List[pd.DataFrame]): holds training pandas dataframe with 2 columns [\"text\",\"label\"]\n",
        "        test (List[pd.DataFrame]): holds testing pandas dataframe with 2 columns [\"text\",\"label\"]\n",
        "        label_list (List[str]): holds the list  of labels\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "        self.label_list = label_list"
      ],
      "metadata": {
        "id": "pL4ChuXHhdEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will hold all the downloaded and structred datasets\n",
        "all_datasets= []\n",
        "DATA_COLUMN = \"text\"\n",
        "LABEL_COLUMN = \"label\""
      ],
      "metadata": {
        "id": "I4S1ghkhhexR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/AIM/fetched_data.csv\" # add the path to the fetched data\n",
        "df = pd.read_csv(\n",
        "    path,\n",
        "    sep=\",\",\n",
        "    header=0,\n",
        ")\n",
        "\n",
        "#df_ASTD_B.columns = [DATA_COLUMN, LABEL_COLUMN]\n",
        "df.columns = [LABEL_COLUMN, DATA_COLUMN]\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "label_list = list(df[LABEL_COLUMN].unique())\n",
        "\n",
        "data = CustomDataset(\n",
        "    \"fetched_data\", train, testB, label_list\n",
        ")\n",
        "all_datasets.append(data)"
      ],
      "metadata": {
        "id": "fShNAn-zhg-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "from arabert.preprocess import ArabertPreprocessor\n",
        "from sklearn.metrics import (accuracy_score, classification_report,\n",
        "                             confusion_matrix, f1_score, precision_score,\n",
        "                             recall_score)\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (AutoConfig, AutoModelForSequenceClassification,\n",
        "                          AutoTokenizer, BertTokenizer, Trainer,\n",
        "                          TrainingArguments)\n",
        "from transformers.data.processors.utils import InputFeatures"
      ],
      "metadata": {
        "id": "KzBwWYi5h8Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select a dataset\n",
        "dataset_name = 'fetched_data'\n",
        "# select a model from the huggingface modelhub https://huggingface.co/models?language=ar\n",
        "model_name = 'aubmindlab/bert-base-arabertv02-twitter' # we are going to use the twitter AraBERT since it has emojis and dialects"
      ],
      "metadata": {
        "id": "sXRD0N8oh_8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arabic_prep = ArabertPreprocessor(model_name)\n",
        "\n",
        "selected_dataset.train[DATA_COLUMN] = selected_dataset.train[DATA_COLUMN].apply(lambda x: arabic_prep.preprocess(x))\n",
        "selected_dataset.test[DATA_COLUMN] = selected_dataset.test[DATA_COLUMN].apply(lambda x: arabic_prep.preprocess(x))  "
      ],
      "metadata": {
        "id": "r7buRTbviD5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "-ULJmWheiHNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Sentence Lengths: \")\n",
        "plt.hist([ len(tok.tokenize(sentence)) for sentence in selected_dataset.train[DATA_COLUMN].to_list()],bins=range(0,128,2))\n",
        "plt.show()\n",
        "\n",
        "print(\"Testing Sentence Lengths: \")\n",
        "plt.hist([ len(tok.tokenize(sentence)) for sentence in selected_dataset.test[DATA_COLUMN].to_list()],bins=range(0,128,2))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cl52ITAiiPQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 80"
      ],
      "metadata": {
        "id": "USGo9qOaiKIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Truncated training sequences: \", sum([len(tok.tokenize(sentence)) > max_len for sentence in selected_dataset.test[DATA_COLUMN].to_list()]))\n",
        "\n",
        "print(\"Truncated testing sequences: \", sum([len(tok.tokenize(sentence)) > max_len for sentence in selected_dataset.test[DATA_COLUMN].to_list()]))"
      ],
      "metadata": {
        "id": "S74qQRIjiNJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationDataset(Dataset):\n",
        "    def __init__(self, text, target, model_name, max_len, label_map):\n",
        "      super(ClassificationDataset).__init__()\n",
        "      \"\"\"\n",
        "      Args:\n",
        "      text (List[str]): List of the training text\n",
        "      target (List[str]): List of the training labels\n",
        "      tokenizer_name (str): The tokenizer name (same as model_name).\n",
        "      max_len (int): Maximum sentence length\n",
        "      label_map (Dict[str,int]): A dictionary that maps the class labels to integer\n",
        "      \"\"\"\n",
        "      self.text = text\n",
        "      self.target = target\n",
        "      self.tokenizer_name = model_name\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      self.max_len = max_len\n",
        "      self.label_map = label_map\n",
        "      \n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.text)\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "      text = str(self.text[item])\n",
        "      text = \" \".join(text.split())\n",
        "        \n",
        "      inputs = self.tokenizer(\n",
        "          text,\n",
        "          max_length=self.max_len,\n",
        "          padding='max_length',\n",
        "          truncation=True\n",
        "      )      \n",
        "      return InputFeatures(**inputs,label=self.label_map[self.target[item]])"
      ],
      "metadata": {
        "id": "wA-BPK7ViV2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = { v:index for index, v in enumerate(selected_dataset.label_list) }\n",
        "print(label_map)\n",
        "\n",
        "train_dataset = ClassificationDataset(\n",
        "    selected_dataset.train[DATA_COLUMN].to_list(),\n",
        "    selected_dataset.train[LABEL_COLUMN].to_list(),\n",
        "    model_name,\n",
        "    max_len,\n",
        "    label_map\n",
        "  )\n",
        "test_dataset = ClassificationDataset(\n",
        "    selected_dataset.test[DATA_COLUMN].to_list(),\n",
        "    selected_dataset.test[LABEL_COLUMN].to_list(),\n",
        "    model_name,\n",
        "    max_len,\n",
        "    label_map\n",
        "  )"
      ],
      "metadata": {
        "id": "K8nhFIEWiZSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_init():\n",
        "    return AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=len(label_map))"
      ],
      "metadata": {
        "id": "OKF64--KiZ2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p): #p should be of type EvalPrediction\n",
        "  preds = np.argmax(p.predictions, axis=1)\n",
        "  assert len(preds) == len(p.label_ids)\n",
        "  #print(classification_report(p.label_ids,preds))\n",
        "  #print(confusion_matrix(p.label_ids,preds))\n",
        "  macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
        "  #macro_precision = precision_score(p.label_ids,preds,average='macro')\n",
        "  #macro_recall = recall_score(p.label_ids,preds,average='macro')\n",
        "  acc = accuracy_score(p.label_ids,preds)\n",
        "  return {       \n",
        "      'macro_f1' : macro_f1,\n",
        "      'accuracy': acc\n",
        "  }"
      ],
      "metadata": {
        "id": "ukCqdQejiciz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic=True\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "n1nb5EEbieBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments( \n",
        "    output_dir= \"./train\",    \n",
        "    adam_epsilon = 1e-8,\n",
        "    learning_rate = 2e-5,\n",
        "    fp16 = False, # enable this when using V100 or T4 GPU\n",
        "    per_device_train_batch_size = 16, # up to 64 on 16GB with max len of 128\n",
        "    per_device_eval_batch_size = 128,\n",
        "    gradient_accumulation_steps = 2, # use this to scale batch size without needing more memory\n",
        "    num_train_epochs= 1,\n",
        "    warmup_ratio = 0,\n",
        "    do_eval = True,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    save_strategy = 'epoch',\n",
        "    load_best_model_at_end = True, # this allows to automatically get the best model at the end based on whatever metric we want\n",
        "    metric_for_best_model = 'macro_f1',\n",
        "    greater_is_better = True,\n",
        "    seed = 25\n",
        "  )\n",
        "\n",
        "set_seed(training_args.seed)"
      ],
      "metadata": {
        "id": "N2iDmcREif4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = model_init(),\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "ChtmHFT1ihkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#start the training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "w9YG6KFyijrO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}